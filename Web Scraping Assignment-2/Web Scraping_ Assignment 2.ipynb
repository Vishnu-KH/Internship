{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baa3d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "\n",
    "# Create a webdriver instance\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open the naukri.com website\n",
    "driver.get('https://www.naukri.com/')\n",
    "\n",
    "# Find the search fields\n",
    "search_fields = driver.find_elements_by_xpath('//div[@class=\"search-fields__container\"]')\n",
    "\n",
    "# Extract the search form\n",
    "search_form = search_fields[0].find_element_by_tag_name('form')\n",
    "\n",
    "# Fill in the search fields and submit the form\n",
    "search_form.find_element_by_id('qsb-keyword-sugg').send_keys('Data Analyst')\n",
    "search_form.find_element_by_id('qsb-location-sugg').send_keys('Bangalore')\n",
    "search_form.submit()\n",
    "\n",
    "# Wait for the search results to load\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# Scrape the job data for the first 10 jobs\n",
    "job_titles = []\n",
    "job_locations = []\n",
    "company_names = []\n",
    "experience_required = []\n",
    "\n",
    "jobs = driver.find_elements_by_xpath('//article[@class=\"jobTuple bgWhite br4 mb-8\"]')\n",
    "\n",
    "for i in range(10):\n",
    "    job = jobs[i]\n",
    "    job_titles.append(job.find_element_by_xpath('.//a[@class=\"title fw500 ellipsis\"]').text)\n",
    "    job_locations.append(job.find_element_by_xpath('.//li[@class=\"fleft grey-text br2 placeHolderLi location\"]/span').text)\n",
    "    company_names.append(job.find_element_by_xpath('.//a[@class=\"subTitle ellipsis fleft\"]').text)\n",
    "    experience_required.append(job.find_element_by_xpath('.//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]/span').text)\n",
    "\n",
    "# Create a pandas dataframe\n",
    "job_data = pd.DataFrame({\n",
    "    'Job Title': job_titles,\n",
    "    'Job Location': job_locations,\n",
    "    'Company Name': company_names,\n",
    "    'Experience Required': experience_required,\n",
    "})\n",
    "\n",
    "print(job_data)\n",
    "\n",
    "# Close the webdriver instance\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2c140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2:Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "\n",
    "# Create a webdriver instance\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open the naukri.com website\n",
    "driver.get('https://www.naukri.com/')\n",
    "\n",
    "# Find the search fields\n",
    "search_fields = driver.find_elements_by_xpath('//div[@class=\"search-fields__container\"]')\n",
    "\n",
    "# Extract the search form\n",
    "search_form = search_fields[0].find_element_by_tag_name('form')\n",
    "\n",
    "# Fill in the search fields and submit the form\n",
    "search_form.find_element_by_id('qsb-keyword-sugg').send_keys('Data Scientist')\n",
    "search_form.find_element_by_id('qsb-location-sugg').send_keys('Bangalore')\n",
    "search_form.submit()\n",
    "\n",
    "# Wait for the search results to load\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# Scrape the job data for the first 10 jobs\n",
    "job_titles = []\n",
    "job_locations = []\n",
    "company_names = []\n",
    "\n",
    "jobs = driver.find_elements_by_xpath('//article[@class=\"jobTuple bgWhite br4 mb-8\"]')\n",
    "\n",
    "for i in range(10):\n",
    "    job = jobs[i]\n",
    "    job_titles.append(job.find_element_by_xpath('.//a[@class=\"title fw500 ellipsis\"]').text)\n",
    "    job_locations.append(job.find_element_by_xpath('.//li[@class=\"fleft grey-text br2 placeHolderLi location\"]/span').text)\n",
    "    company_names.append(job.find_element_by_xpath('.//a[@class=\"subTitle ellipsis fleft\"]').text)\n",
    "\n",
    "# Create a pandas dataframe\n",
    "job_data = pd.DataFrame({\n",
    "    'Job Title': job_titles,\n",
    "    'Job Location': job_locations,\n",
    "    'Company Name': company_names,\n",
    "})\n",
    "\n",
    "print(job_data)\n",
    "\n",
    "# Close the webdriver instance\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c874d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3:\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "# specify the path to chromedriver.exe (download and save on your computer)\n",
    "driver = webdriver.Chrome('path/to/chromedriver.exe')\n",
    "\n",
    "# get the naukri.com url\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "\n",
    "# find the search field for job designation and enter \"Data Scientist\"\n",
    "search_job = driver.find_element_by_name(\"keyword\")\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "\n",
    "# find the search button and click it\n",
    "search_btn = driver.find_element_by_css_selector(\"button.search-btn\")\n",
    "search_btn.click()\n",
    "\n",
    "# wait for the page to load\n",
    "wait = WebDriverWait(driver, 10)\n",
    "wait.until(EC.presence_of_element_located((By.ID, \"root\")))\n",
    "\n",
    "# select the location filter for \"Delhi/NCR\"\n",
    "location_filter = driver.find_element_by_xpath(\"//label[@for='chk-Delhi/NCR-cityTypeGid-']/i\")\n",
    "location_filter.click()\n",
    "\n",
    "# select the salary filter for \"3-6 lakhs\"\n",
    "salary_filter = driver.find_element_by_xpath(\"//label[@for='chk-3-6 Lakhs-ctcFilter-']/i\")\n",
    "salary_filter.click()\n",
    "\n",
    "# wait for the page to reload with the applied filters\n",
    "wait.until(EC.presence_of_element_located((By.ID, \"root\")))\n",
    "\n",
    "# scrape the job data for the first 10 job results\n",
    "job_data = []\n",
    "for i in range(1, 11):\n",
    "    # get the job title\n",
    "    title = driver.find_element_by_xpath(f\"//article[{i}]/div[1]/a\").text\n",
    "    \n",
    "    # get the job location\n",
    "    location = driver.find_element_by_xpath(f\"//article[{i}]/div[1]/ul/li[3]\").text\n",
    "    \n",
    "    # get the company name\n",
    "    company = driver.find_element_by_xpath(f\"//article[{i}]/div[1]/a[2]\").text\n",
    "    \n",
    "    # get the experience required\n",
    "    exp = driver.find_element_by_xpath(f\"//article[{i}]/div[1]/ul/li[1]\").text\n",
    "    \n",
    "    # add the job data to the list\n",
    "    job_data.append([title, location, company, exp])\n",
    "    \n",
    "# create a pandas dataframe with the scraped job data\n",
    "df = pd.DataFrame(job_data, columns=[\"Job Title\", \"Job Location\", \"Company Name\", \"Experience Required\"])\n",
    "\n",
    "# print the dataframe\n",
    "print(df)\n",
    "\n",
    "# close the webdriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6022475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4:\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "# Set up the driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Go to Flipkart homepage\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "\n",
    "# Find the search bar element and enter \"sunglasses\"\n",
    "search_bar = driver.find_element(By.NAME, \"q\")\n",
    "search_bar.send_keys(\"sunglasses\")\n",
    "\n",
    "# Click the search button\n",
    "search_button = driver.find_element(By.CLASS_NAME, \"L0Z3Pu\")\n",
    "search_button.click()\n",
    "\n",
    "# Loop through the pages until 100 sunglasses have been scraped\n",
    "sunglasses_data = []\n",
    "while len(sunglasses_data) < 100:\n",
    "    # Wait for page to load\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Get the sunglasses data from the page\n",
    "    sunglasses = driver.find_elements(By.XPATH, \"//div[@class='_2kHMtA']\")\n",
    "    for sunglass in sunglasses:\n",
    "        brand = sunglass.find_element(By.XPATH, \".//div[1]\").text\n",
    "        product_description = sunglass.find_element(By.XPATH, \".//a[1]\").text\n",
    "        price = sunglass.find_element(By.XPATH, \".//div[2]/div[1]\").text\n",
    "        \n",
    "        sunglasses_data.append({\n",
    "            \"Brand\": brand,\n",
    "            \"Product Description\": product_description,\n",
    "            \"Price\": price\n",
    "        })\n",
    "        \n",
    "        # Break the loop if 100 sunglasses have been scraped\n",
    "        if len(sunglasses_data) >= 100:\n",
    "            break\n",
    "        \n",
    "    # Go to the next page\n",
    "    next_button = driver.find_element(By.XPATH, \"//a[@class='_1LKTO3']//span[text()='Next']\")\n",
    "    next_button.click()\n",
    "    \n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "# Print the scraped data\n",
    "for i, sunglass in enumerate(sunglasses_data, 1):\n",
    "    print(f\"-----Sunglass {i}-----\")\n",
    "    print(f\"Brand: {sunglass['Brand']}\")\n",
    "    print(f\"Product Description: {sunglass['Product Description']}\")\n",
    "    print(f\"Price: {sunglass['Price']}\")\n",
    "    print(\"---------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b2df40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5:\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# configure webdriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "# create webdriver instance\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# navigate to the page\n",
    "driver.get('https://www.flipkart.com/apple-iphone-11-black-64-gb/productreviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART')\n",
    "\n",
    "# scrape reviews\n",
    "reviews = []\n",
    "count = 0\n",
    "while count < 100:\n",
    "    # wait for reviews to load\n",
    "    try:\n",
    "        element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, 'div._1AtVbE.col-12-12'))\n",
    "        )\n",
    "    except:\n",
    "        break\n",
    "    \n",
    "    # scrape review attributes\n",
    "    for review in driver.find_elements(By.CSS_SELECTOR, 'div._1AtVbE.col-12-12'):\n",
    "        if count >= 100:\n",
    "            break\n",
    "        rating = review.find_element(By.CSS_SELECTOR, 'div._3LWZlK').text\n",
    "        summary = review.find_element(By.CSS_SELECTOR, 'p._2-N8zT').text\n",
    "        full_review = review.find_element(By.CSS_SELECTOR, 'div.t-ZTKy').text\n",
    "        reviews.append({\n",
    "            'rating': rating,\n",
    "            'summary': summary,\n",
    "            'full_review': full_review\n",
    "        })\n",
    "        count += 1\n",
    "    \n",
    "    # navigate to next page\n",
    "    try:\n",
    "        next_button = driver.find_element(By.CSS_SELECTOR, 'a._1LKTO3')\n",
    "        driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "    except:\n",
    "        break\n",
    "\n",
    "# print reviews\n",
    "for review in reviews:\n",
    "    print(review)\n",
    "\n",
    "# close webdriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347b5a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6:\n",
    "\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "\n",
    "# Set up Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Load the website\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "\n",
    "# Close login popup if it appears\n",
    "try:\n",
    "    driver.find_element_by_xpath('//button[text()=\"✕\"]').click()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Search for \"sneakers\"\n",
    "search_box = driver.find_element_by_name(\"q\")\n",
    "search_box.send_keys(\"sneakers\")\n",
    "search_box.submit()\n",
    "\n",
    "# Set up empty lists for data\n",
    "brands = []\n",
    "descriptions = []\n",
    "prices = []\n",
    "\n",
    "# Scrape the first 100 sneakers\n",
    "while len(prices) < 100:\n",
    "    # Find all the products on the page\n",
    "    products = driver.find_elements_by_xpath('//div[@class=\"_2kHMtA\"]')\n",
    "    \n",
    "    # Scrape data for each product\n",
    "    for product in products:\n",
    "        # Get the brand name\n",
    "        try:\n",
    "            brand = product.find_element_by_xpath('.//div[1]/a[1]').text\n",
    "        except:\n",
    "            brand = \"Unknown\"\n",
    "        brands.append(brand)\n",
    "        \n",
    "        # Get the product description\n",
    "        try:\n",
    "            description = product.find_element_by_xpath('.//a[1]/div[1]').text\n",
    "        except:\n",
    "            description = \"Unknown\"\n",
    "        descriptions.append(description)\n",
    "        \n",
    "        # Get the price\n",
    "        try:\n",
    "            price = product.find_element_by_xpath('.//div[1]/a[2]').text\n",
    "        except:\n",
    "            price = \"Unknown\"\n",
    "        prices.append(price)\n",
    "        \n",
    "        # Break out of the loop if we have enough data\n",
    "        if len(prices) >= 100:\n",
    "            break\n",
    "    \n",
    "    # Go to the next page\n",
    "    try:\n",
    "        next_button = driver.find_element_by_xpath('//a[text()=\"Next\"]')\n",
    "        driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "    except:\n",
    "        break\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "# Create a dataframe of the scraped data\n",
    "sneakers_df = pd.DataFrame({'Brand': brands[:100], 'Description': descriptions[:100], 'Price': prices[:100]})\n",
    "\n",
    "# Print the dataframe\n",
    "print(sneakers_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbffe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7: \n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "# Launch the Chrome browser and open the Amazon website\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "\n",
    "# Find the search box and enter the search query\n",
    "search_box = driver.find_element_by_id(\"twotabsearchtextbox\")\n",
    "search_box.send_keys(\"Laptop\")\n",
    "\n",
    "# Click the search button\n",
    "search_button = driver.find_element_by_xpath(\"//input[@value='Go']\")\n",
    "search_button.click()\n",
    "\n",
    "# Wait for the CPU Type filter to be loaded and click on it\n",
    "cpu_filter = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.LINK_TEXT, \"Intel Core i7\")))\n",
    "cpu_filter.click()\n",
    "\n",
    "# Wait for the laptops to be loaded and scrape the data for the first 10 laptops\n",
    "laptops = WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.XPATH, \"//div[@data-component-type='s-search-result']\")))\n",
    "laptop_data = []\n",
    "for laptop in laptops[:10]:\n",
    "    # Scrape the title\n",
    "    title_element = laptop.find_element_by_xpath(\".//h2/a\")\n",
    "    title = title_element.text.strip()\n",
    "\n",
    "    # Scrape the rating\n",
    "    try:\n",
    "        rating_element = laptop.find_element_by_xpath(\".//span[@class='a-icon-alt']\")\n",
    "        rating = rating_element.get_attribute(\"innerHTML\")\n",
    "    except:\n",
    "        rating = \"Not available\"\n",
    "\n",
    "    # Scrape the price\n",
    "    try:\n",
    "        price_element = laptop.find_element_by_xpath(\".//span[@class='a-price-whole']\")\n",
    "        price = price_element.text.strip()\n",
    "    except:\n",
    "        price = \"Not available\"\n",
    "\n",
    "    laptop_data.append({\n",
    "        \"Title\": title,\n",
    "        \"Rating\": rating,\n",
    "        \"Price\": price\n",
    "    })\n",
    "\n",
    "# Create a pandas DataFrame with the scraped data\n",
    "df = pd.DataFrame(laptop_data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7437d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8:\n",
    "\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "\n",
    "# set up the webdriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# navigate to the webpage\n",
    "driver.get('https://www.azquotes.com/')\n",
    "\n",
    "# click on TopQuotes\n",
    "top_quotes = driver.find_element_by_link_text('Top Quotes')\n",
    "top_quotes.click()\n",
    "\n",
    "# set up empty lists to store the data\n",
    "quotes = []\n",
    "authors = []\n",
    "types = []\n",
    "\n",
    "# scrape the data for the first 1000 quotes\n",
    "for i in range(1, 1001):\n",
    "    # get the quote\n",
    "    quote = driver.find_element_by_xpath(f'//*[@id=\"quotes_list\"]/div[{i}]/div[1]')\n",
    "    quotes.append(quote.text)\n",
    "    \n",
    "    # get the author\n",
    "    author = driver.find_element_by_xpath(f'//*[@id=\"quotes_list\"]/div[{i}]/div[2]/div[1]/a')\n",
    "    authors.append(author.text)\n",
    "    \n",
    "    # get the type\n",
    "    quote_type = driver.find_element_by_xpath(f'//*[@id=\"quotes_list\"]/div[{i}]/div[2]/div[1]/div[1]')\n",
    "    types.append(quote_type.text)\n",
    "    \n",
    "# create a dataframe of the scraped data\n",
    "data = {'Quote': quotes, 'Author': authors, 'Type': types}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# print the dataframe\n",
    "print(df)\n",
    "\n",
    "# close the webdriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8633fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9:\n",
    "\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "\n",
    "# set up Chrome webdriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# navigate to webpage\n",
    "driver.get('https://www.jagranjosh.com/')\n",
    "\n",
    "# click on GK option\n",
    "gk_option = driver.find_element_by_xpath('//a[@title=\"General Knowledge\"]')\n",
    "gk_option.click()\n",
    "\n",
    "# click on List of all Prime Ministers of India\n",
    "pm_list_option = driver.find_element_by_xpath('//a[text()=\"List of all Prime Ministers of India\"]')\n",
    "pm_list_option.click()\n",
    "\n",
    "# get table data\n",
    "table = driver.find_element_by_xpath('//table[@class=\"content-table\"]')\n",
    "rows = table.find_elements_by_xpath('.//tr')\n",
    "\n",
    "# create empty list to store data\n",
    "data = []\n",
    "\n",
    "# iterate through rows and get data for each former PM\n",
    "for row in rows[1:]:\n",
    "    columns = row.find_elements_by_xpath('.//td')\n",
    "    name = columns[0].text\n",
    "    born_dead = columns[1].text\n",
    "    term_of_office = columns[2].text\n",
    "    remarks = columns[3].text\n",
    "    \n",
    "    # append data to list\n",
    "    data.append({'Name': name, 'Born-Dead': born_dead, 'Term of Office': term_of_office, 'Remarks': remarks})\n",
    "\n",
    "# create dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# display dataframe\n",
    "print(df)\n",
    "\n",
    "# close the webdriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f76f34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q10:\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "# Set up the driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Load the webpage\n",
    "driver.get(\"https://www.motor1.com/\")\n",
    "\n",
    "# Wait for the List option to be visible and click it\n",
    "list_option = WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.LINK_TEXT, \"List\")))\n",
    "list_option.click()\n",
    "\n",
    "# Wait for the 50 most expensive cars option to be visible and click it\n",
    "expensive_cars_option = WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.LINK_TEXT, \"50 Most Expensive Cars\")))\n",
    "expensive_cars_option.click()\n",
    "\n",
    "# Scrape the car name and price data\n",
    "car_names = []\n",
    "prices = []\n",
    "car_data = driver.find_elements(By.CSS_SELECTOR, \".widget--card-item\")\n",
    "for car in car_data:\n",
    "    car_names.append(car.find_element(By.CSS_SELECTOR, \".widget--title\").text)\n",
    "    prices.append(car.find_element(By.CSS_SELECTOR, \".widget--price\").text)\n",
    "\n",
    "# Create a dataframe of the scraped data\n",
    "data = {\"Car Name\": car_names, \"Price\": prices}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the dataframe\n",
    "print(df)\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
