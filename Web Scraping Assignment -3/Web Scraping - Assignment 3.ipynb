{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a9e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Write a python program which searches all the product under a particular product from www.amazon.in. The\n",
    "product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for\n",
    "guitars'''\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon(product):\n",
    "    # Set up the URL and query parameters\n",
    "    url = 'https://www.amazon.in/s'\n",
    "    params = {'k': product}\n",
    "    \n",
    "    # Send a GET request to Amazon with the query parameters\n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all the product listings on the page\n",
    "    products = soup.find_all('div', {'class': 's-result-item'})\n",
    "    \n",
    "    # Extract the relevant information from each product listing\n",
    "    for product in products:\n",
    "        # Get the product title\n",
    "        title = product.find('h2').text.strip()\n",
    "        \n",
    "        # Get the product price, if available\n",
    "        price_element = product.find('span', {'class': 'a-price-whole'})\n",
    "        price = price_element.text.strip() if price_element else 'N/A'\n",
    "        \n",
    "        # Get the product rating, if available\n",
    "        rating_element = product.find('span', {'class': 'a-icon-alt'})\n",
    "        rating = rating_element.text.strip() if rating_element else 'N/A'\n",
    "        \n",
    "        # Print the information for the product\n",
    "        print(f'{title}\\nPrice: {price}\\nRating: {rating}\\n')\n",
    "\n",
    "search_amazon('guitar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c8148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''In the above question, now scrape the following details of each product listed in first 3 pages of your search\n",
    "results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then\n",
    "scrape all the products available under that product name. Details to be scraped are: \"Brand\n",
    "Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and\n",
    "“Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“ '''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_products(product, pages):\n",
    "    # Set up the URL and query parameters\n",
    "    url = 'https://www.amazon.in/s'\n",
    "    params = {'k': product}\n",
    "    \n",
    "    # Create an empty list to store the data for each product\n",
    "    products_data = []\n",
    "    \n",
    "    # Iterate over the specified number of pages of search results\n",
    "    for page in range(1, pages+1):\n",
    "        params['page'] = page\n",
    "        \n",
    "        # Send a GET request to Amazon with the query parameters\n",
    "        response = requests.get(url, params=params)\n",
    "        \n",
    "        # Parse the HTML content of the page using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all the product listings on the page\n",
    "        products = soup.find_all('div', {'class': 's-result-item'})\n",
    "        \n",
    "        # Extract the relevant information from each product listing\n",
    "        for product in products:\n",
    "            # Get the product title and URL\n",
    "            title_element = product.find('h2')\n",
    "            if title_element:\n",
    "                title = title_element.text.strip()\n",
    "                url = 'https://www.amazon.in' + title_element.find('a')['href']\n",
    "            else:\n",
    "                title = '-'\n",
    "                url = '-'\n",
    "            \n",
    "            # Get the product brand and name\n",
    "            brand_element = product.find('span', {'class': 'a-size-base-plus'})\n",
    "            if brand_element:\n",
    "                brand_name = brand_element.text.strip().split(' ')[0]\n",
    "                name = ' '.join(brand_element.text.strip().split(' ')[1:])\n",
    "            else:\n",
    "                brand_name = '-'\n",
    "                name = title\n",
    "            \n",
    "            # Get the product price and availability\n",
    "            price_element = product.find('span', {'class': 'a-price-whole'})\n",
    "            if price_element:\n",
    "                price = price_element.text.strip()\n",
    "                availability = product.find('span', {'class': 'a-size-medium a-color-success'}).text.strip()\n",
    "            else:\n",
    "                price = '-'\n",
    "                availability = '-'\n",
    "            \n",
    "            # Get the product return/exchange policy\n",
    "            return_element = product.find('div', {'class': 'a-section a-spacing-none'})\n",
    "            if return_element:\n",
    "                return_policy = return_element.find('div', {'class': 'a-row a-size-small'}).text.strip()\n",
    "            else:\n",
    "                return_policy = '-'\n",
    "            \n",
    "            # Get the product expected delivery date\n",
    "            delivery_element = product.find('div', {'class': 'a-section a-spacing-base a-text-center'})\n",
    "            if delivery_element:\n",
    "                delivery_text = delivery_element.find_all('div')[1].text.strip()\n",
    "                expected_delivery = delivery_text.replace('Delivery by ', '')\n",
    "            else:\n",
    "                expected_delivery = '-'\n",
    "            \n",
    "            # Add the data for this product to the list\n",
    "            products_data.append({\n",
    "                'Brand Name': brand_name,\n",
    "                'Name of the Product': name,\n",
    "                'Price': price,\n",
    "                'Return/Exchange': return_policy,\n",
    "                'Expected Delivery': expected_delivery,\n",
    "                'Availability': availability,\n",
    "                'Product URL': url\n",
    "            })\n",
    "    \n",
    "    # Create a pandas DataFrame from the products data\n",
    "    df = pd.DataFrame(products_data)\n",
    "    \n",
    "    # Save the data to a CSV file\n",
    "    df.to_csv(f'{product}_products.csv', index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479bd884",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Write a python program to access the search bar and search button on images.google.com and scrape 10\n",
    "images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’ '''\n",
    "\n",
    "from selenium import webdriver\n",
    "import urllib.request\n",
    "import time\n",
    "\n",
    "# create a webdriver object and set options\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--incognito')\n",
    "options.add_argument('--headless')\n",
    "\n",
    "# set the path to the chromedriver.exe file (make sure to download and place it in the working directory)\n",
    "driver_path = './chromedriver.exe'\n",
    "driver = webdriver.Chrome(driver_path, options=options)\n",
    "\n",
    "# navigate to images.google.com\n",
    "driver.get('https://www.google.com/imghp')\n",
    "\n",
    "# locate the search bar and search button\n",
    "search_bar = driver.find_element_by_name('q')\n",
    "search_button = driver.find_element_by_css_selector('button[jsaction=\"click:trigger.search\"]')\n",
    "\n",
    "# define the search terms\n",
    "search_terms = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "\n",
    "# loop through the search terms and scrape 10 images for each term\n",
    "for term in search_terms:\n",
    "    # enter the search term in the search bar and click the search button\n",
    "    search_bar.send_keys(term)\n",
    "    search_button.click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    # scroll down to load more images\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # scrape the image urls and download the images\n",
    "    image_urls = driver.find_elements_by_css_selector('img.rg_i')\n",
    "    count = 1\n",
    "    for image in image_urls[:10]:\n",
    "        try:\n",
    "            image_url = image.get_attribute('src')\n",
    "            urllib.request.urlretrieve(image_url, f'{term}_{count}.jpg')\n",
    "            print(f'{term} image {count} downloaded')\n",
    "            count += 1\n",
    "        except:\n",
    "            print(f'Error downloading {term} image {count}')\n",
    "            count += 1\n",
    "\n",
    "    # clear the search bar for the next search term\n",
    "    search_bar.clear()\n",
    "\n",
    "# close the webdriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af287c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com\n",
    "and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand\n",
    "Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”,\n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”'''\n",
    "\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "# create a webdriver object and set options\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--incognito')\n",
    "options.add_argument('--headless')\n",
    "\n",
    "# set the path to the chromedriver.exe file (make sure to download and place it in the working directory)\n",
    "driver_path = './chromedriver.exe'\n",
    "driver = webdriver.Chrome(driver_path, options=options)\n",
    "\n",
    "# navigate to flipkart.com\n",
    "driver.get('https://www.flipkart.com/')\n",
    "\n",
    "# close the login popup if it appears\n",
    "try:\n",
    "    driver.find_element_by_xpath('/html/body/div[2]/div/div/button').click()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# locate the search bar and search button\n",
    "search_bar = driver.find_element_by_name('q')\n",
    "search_button = driver.find_element_by_xpath('/html/body/div/div/div[1]/div[1]/div[2]/div[2]/form/div/button')\n",
    "\n",
    "# enter the search term and click the search button\n",
    "search_term = 'Oneplus Nord' # change this to the desired smartphone name\n",
    "search_bar.send_keys(search_term)\n",
    "search_button.click()\n",
    "time.sleep(2)\n",
    "\n",
    "# loop through the search results and scrape the required details\n",
    "results = driver.find_elements_by_css_selector('div._2kHMtA')\n",
    "for result in results:\n",
    "    try:\n",
    "        brand = result.find_element_by_css_selector('div._2WkVRV').text\n",
    "        name = result.find_element_by_css_selector('a._1fQZEK').text\n",
    "        url = result.find_element_by_css_selector('a._1fQZEK').get_attribute('href')\n",
    "        details = result.find_element_by_css_selector('ul._1xgFaf')\n",
    "        color = details.find_elements_by_css_selector('li')[0].text\n",
    "        ram = details.find_elements_by_css_selector('li')[1].text\n",
    "        rom = details.find_elements_by_css_selector('li')[2].text\n",
    "        camera = details.find_elements_by_css_selector('li')[3].text\n",
    "        front_camera = details.find_elements_by_css_selector('li')[4].text\n",
    "        display = details.find_elements_by_css_selector('li')[5].text\n",
    "        battery = details.find_elements_by_css_selector('li')[6].text\n",
    "        price = result.find_element_by_css_selector('div._30jeq3._1_WHN1').text\n",
    "        print(f'Brand: {brand}\\nName: {name}\\nColor: {color}\\nRAM: {ram}\\nROM: {rom}\\nPrimary Camera: {camera}\\nSecondary Camera: {front_camera}\\nDisplay Size: {display}\\nBattery Capacity: {battery}\\nPrice: {price}\\nProduct URL: {url}\\n')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# close the webdriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e0b31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ask user to enter the city name\n",
    "city = input('Enter the name of the city: ')\n",
    "\n",
    "# send a GET request to Google Maps with the city name as the search query\n",
    "url = f'https://www.google.com/maps/search/{city}'\n",
    "response = requests.get(url)\n",
    "\n",
    "# parse the HTML response using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# locate the div containing the coordinates and extract the latitude and longitude\n",
    "coordinates_div = soup.find('div', class_='ugiz4pqJLAG__primary-text gm2-body-2')\n",
    "coordinates = coordinates_div.text.split(',')\n",
    "\n",
    "latitude = coordinates[0].strip()\n",
    "longitude = coordinates[1].strip()\n",
    "\n",
    "# print the latitude and longitude\n",
    "print(f'Latitude: {latitude}\\nLongitude: {longitude}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b48689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a program to scrap all the available details of best gaming laptops from digit.in\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# send a GET request to the URL of the webpage we want to scrape\n",
    "url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# create soup object to parse HTML\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# find the section containing all the laptops\n",
    "laptops = soup.find_all('div', class_='TopNumbeHeading sticky-footer')\n",
    "\n",
    "# create lists to store the laptop details\n",
    "names = []\n",
    "prices = []\n",
    "ratings = []\n",
    "specs = []\n",
    "\n",
    "# iterate over each laptop and extract details\n",
    "for laptop in laptops:\n",
    "    # extract name, price, and rating\n",
    "    name = laptop.a.text.strip()\n",
    "    price = laptop.find('div', class_='smprice').text.strip()[1:]\n",
    "    rating = laptop.find('div', class_='rating').text.strip()\n",
    "    \n",
    "    # extract specs\n",
    "    url = laptop.a['href']\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    specs_dict = {}\n",
    "    specs_table = soup.find('div', class_='specifications_table')\n",
    "    rows = specs_table.find_all('tr')\n",
    "    for row in rows:\n",
    "        key = row.find('td', class_='smkey').text.strip()\n",
    "        value = row.find('td', class_='smvalue').text.strip()\n",
    "        specs_dict[key] = value\n",
    "    \n",
    "    # add details to lists\n",
    "    names.append(name)\n",
    "    prices.append(price)\n",
    "    ratings.append(rating)\n",
    "    specs.append(specs_dict)\n",
    "\n",
    "# print the details of each laptop\n",
    "for i in range(len(laptops)):\n",
    "    print(f\"Laptop {i+1}:\")\n",
    "    print(f\"Name: {names[i]}\")\n",
    "    print(f\"Price: {prices[i]}\")\n",
    "    print(f\"Rating: {ratings[i]}\")\n",
    "    print(\"Specifications:\")\n",
    "    for key, value in specs[i].items():\n",
    "        print(f\"\\t{key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d63474",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped:\n",
    "“Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”'''\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# send a GET request to the URL of the webpage we want to scrape\n",
    "url = \"https://www.forbes.com/billionaires/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# create soup object to parse HTML\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# find the table containing all the billionaires\n",
    "table = soup.find('table', class_='table')\n",
    "\n",
    "# create lists to store the billionaire details\n",
    "ranks = []\n",
    "names = []\n",
    "net_worths = []\n",
    "ages = []\n",
    "citizenships = []\n",
    "sources = []\n",
    "industries = []\n",
    "\n",
    "# iterate over each row in the table and extract details\n",
    "rows = table.find_all('tr')\n",
    "for row in rows[1:]:\n",
    "    # extract rank, name, and net worth\n",
    "    rank = row.find('td', class_='rank').text.strip()\n",
    "    name = row.find('td', class_='name').text.strip()\n",
    "    net_worth = row.find('td', class_='netWorth').text.strip()[1:]\n",
    "    \n",
    "    # extract age, citizenship, source, and industry\n",
    "    columns = row.find_all('td', class_='')\n",
    "    age = columns[2].text.strip()\n",
    "    citizenship = columns[3].text.strip()\n",
    "    source = columns[4].text.strip()\n",
    "    industry = columns[5].text.strip()\n",
    "    \n",
    "    # add details to lists\n",
    "    ranks.append(rank)\n",
    "    names.append(name)\n",
    "    net_worths.append(net_worth)\n",
    "    ages.append(age)\n",
    "    citizenships.append(citizenship)\n",
    "    sources.append(source)\n",
    "    industries.append(industry)\n",
    "\n",
    "# print the details of each billionaire\n",
    "for i in range(len(ranks)):\n",
    "    print(f\"Billionaire {i+1}:\")\n",
    "    print(f\"Rank: {ranks[i]}\")\n",
    "    print(f\"Name: {names[i]}\")\n",
    "    print(f\"Net worth: {net_worths[i]}\")\n",
    "    print(f\"Age: {ages[i]}\")\n",
    "    print(f\"Citizenship: {citizenships[i]}\")\n",
    "    print(f\"Source: {sources[i]}\")\n",
    "    print(f\"Industry: {industries[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523132a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video. \n",
    "\n",
    "pip install google-auth google-auth-oauthlib\n",
    "\n",
    "from google.oauth2.credentials import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# Replace with your own API key\n",
    "API_KEY = \"YOUR_API_KEY_HERE\"\n",
    "\n",
    "# Set up the YouTube Data API client\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Get the video ID of the Leo Promo video\n",
    "video_id = \"VIDEO_ID_HERE\"\n",
    "\n",
    "# Get the comment thread iterator\n",
    "def get_comment_threads(video_id):\n",
    "    try:\n",
    "        response = youtube.commentThreads().list(\n",
    "            part=\"snippet\",\n",
    "            videoId=video_id,\n",
    "            textFormat=\"plainText\",\n",
    "            maxResults=100\n",
    "        ).execute()\n",
    "        yield response\n",
    "        while 'nextPageToken' in response:\n",
    "            time.sleep(1)\n",
    "            response = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                textFormat=\"plainText\",\n",
    "                maxResults=100,\n",
    "                pageToken=response['nextPageToken']\n",
    "            ).execute()\n",
    "            yield response\n",
    "    except HttpError as e:\n",
    "        print(f\"An HTTP error {e.resp.status} occurred:\\n{e.content}\")\n",
    "\n",
    "# Get the comments and their metadata\n",
    "comments = []\n",
    "for thread in get_comment_threads(video_id):\n",
    "    for item in thread['items']:\n",
    "        comment = item['snippet']['topLevelComment']['snippet']\n",
    "        comment_data = {\n",
    "            \"text\": comment['textDisplay'],\n",
    "            \"upvotes\": comment['likeCount'],\n",
    "            \"timestamp\": datetime.strptime(comment['publishedAt'], '%Y-%m-%dT%H:%M:%S%z').strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        comments.append(comment_data)\n",
    "        if len(comments) >= 500:\n",
    "            break\n",
    "    if len(comments) >= 500:\n",
    "        break\n",
    "\n",
    "# Print the comments\n",
    "for comment in comments:\n",
    "    print(f\"{comment['timestamp']} - {comment['upvotes']} upvotes\\n{comment['text']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b21ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in\n",
    "“London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall\n",
    "reviews, privates from price, dorms from price, facilities and property description'''\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL for the search results page\n",
    "url = \"https://www.hostelworld.com/search?search_keywords=London,%20England&country=England&city=London&date_from=2023-03-10&date_to=2023-03-13&number_of_guests=1\"\n",
    "\n",
    "# Send a GET request to the URL and get the response\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the response using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all the hostel listings on the search results page\n",
    "hostel_listings = soup.find_all(\"div\", class_=\"property-card\")\n",
    "\n",
    "# Loop over the hostel listings and extract the data we want\n",
    "for listing in hostel_listings:\n",
    "    # Get the hostel name\n",
    "    hostel_name = listing.find(\"h2\", class_=\"title\").text.strip()\n",
    "\n",
    "    # Get the distance from the city center\n",
    "    distance = listing.find(\"span\", class_=\"description\").text.strip()\n",
    "\n",
    "    # Get the overall rating and total reviews\n",
    "    rating = listing.find(\"div\", class_=\"score orange big\").text.strip()\n",
    "    total_reviews = listing.find(\"div\", class_=\"reviews\").text.strip()\n",
    "\n",
    "    # Get the prices for privates and dorms\n",
    "    prices = listing.find(\"div\", class_=\"price-col\")\n",
    "    private_price = prices.find(\"span\", class_=\"price\").text.strip()\n",
    "    dorm_price = prices.find(\"span\", class_=\"price\").find_next_sibling().text.strip()\n",
    "\n",
    "    # Get the hostel facilities\n",
    "    facilities = listing.find(\"div\", class_=\"facilities-label\").find_next_sibling().text.strip()\n",
    "\n",
    "    # Get the property description\n",
    "    description = listing.find(\"div\", class_=\"property-description\").text.strip()\n",
    "\n",
    "    # Print the hostel data\n",
    "    print(\"Hostel Name:\", hostel_name)\n",
    "    print(\"Distance from City Center:\", distance)\n",
    "    print(\"Overall Rating:\", rating)\n",
    "    print(\"Total Reviews:\", total_reviews)\n",
    "    print(\"Private Room Prices:\", private_price)\n",
    "    print(\"Dorm Room Prices:\", dorm_price)\n",
    "    print(\"Facilities:\", facilities)\n",
    "    print(\"Description:\", description)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
